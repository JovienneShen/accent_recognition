# Generated 2025-03-20 from:
# C:\Users\junwe\Desktop\accent_recog\speechbrain\accent-recog-slt2022\CommonAccent\accent_id\hparams\inference_ecapa_tdnn_en_us_70k.yaml
# yamllint disable
# #################################
# Training ECAPA-TDNN embeddings for Accent identification (LID) of English Data.
#
# Authors:
#  * For SLT Juan Pablo and Sara
# #################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:torch.manual_seed [1986]

# Set up folders for reading from and writing:
data_folder: 
  C:\Users\junwe\Desktop\accent_recog\speechbrain\accent-recog-slt2022\CommonAccent
csv_prepared_folder: data/en_us_70k/
output_folder: results/analysis_ECAPA-TDNN-241229_70k_filtered-0.7
pretrained_path: 
  C:\Users\junwe\Desktop\accent_recog\speechbrain\accent-recog-slt2022\CommonAccent\accent_id\results\ECAPA-TDNN-241229-70k-filtered-0.7\1986\save\CKPT+2025-03-19+07-19-41+00
device: cuda:0
skip_prep: true
# max audio lenth allowed, higher than this is dropped
max_audio_length: 10

# Feature parameters btw: 40 - 80
n_mels: 80

# Training Parameters
sample_rate: 16000
batch_size: 64
n_languages: 2
emb_dim: 192 # dimensionality of the embeddings

test_dataloader_options:
  batch_size: 32
  shuffle: true

# variable to control whether to apply augmentation or not:
apply_augmentation: false

# Feature extraction
compute_features: &id001 !new:speechbrain.lobes.features.Fbank
  n_mels: 80

# Mean and std normalization of the input features
mean_var_norm: &id002 !new:speechbrain.processing.features.InputNormalization
  norm_type: sentence
  std_norm: false

# To design a custom model, either just edit the simple CustomModel
# class that's listed here, or replace this `!new` call with a line
# pointing to a different file you've defined.

# Embedding Model
embedding_model: &id003 !new:speechbrain.lobes.models.ECAPA_TDNN.ECAPA_TDNN
  input_size: 80
  activation: !name:torch.nn.LeakyReLU
  channels: [1024, 1024, 1024, 1024, 3072]
  kernel_sizes: [5, 3, 3, 3, 1]
  dilations: [1, 2, 3, 4, 1]
  attention_channels: 128
  lin_neurons: 192

# Classifier based on cosine distance
classifier: &id004 !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier

  input_size: 192
  out_neurons: 2

# Loss function to calculate during inference (this is optional)
# Additive Angular Margin
compute_cost: !new:speechbrain.nnet.losses.LogSoftmaxWrapper
  loss_fn: !new:speechbrain.nnet.losses.AdditiveAngularMargin
    margin: 0.2
    scale: 30

# Keep the error stats here:
error_stats: !name:speechbrain.utils.metric_stats.MetricStats
  metric: !name:speechbrain.nnet.losses.classification_error
    reduction: batch

# Keep the error stats here:
error_stats2: !name:speechbrain.utils.metric_stats.BinaryMetricStats

# Objects in "modules" dict will have their parameters moved to the correct
# device, as well as having train()/eval() called on them by the Brain class.
modules:
  compute_features: *id001
  mean_var_norm: *id002
  embedding_model: *id003
  classifier: *id004
label_encoder: &id005 !new:speechbrain.dataio.encoder.CategoricalEncoder

# Load fine-tuned model and classifier
pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
  loadables:
        # embedding_model: !ref <embedding_model>
        # classifier: !ref <classifier>
    label_encoder: *id005
    mean_var_norm: *id002
  paths:
        # embedding_model: !ref <pretrained_path>/embedding_model.ckpt
        # classifier: !ref <pretrained_path>/classifier.ckpt
    label_encoder: 
      C:\Users\junwe\Desktop\accent_recog\speechbrain\accent-recog-slt2022\CommonAccent\accent_id\results\ECAPA-TDNN-241229-70k-filtered-0.7\1986\save\CKPT+2025-03-19+07-19-41+00/accent_encoder.txt
    mean_var_norm: 
      C:\Users\junwe\Desktop\accent_recog\speechbrain\accent-recog-slt2022\CommonAccent\accent_id\results\ECAPA-TDNN-241229-70k-filtered-0.7\1986\save\CKPT+2025-03-19+07-19-41+00/normalizer_input.ckpt
